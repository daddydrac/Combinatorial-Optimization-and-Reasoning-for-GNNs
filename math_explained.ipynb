{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Below is a rewritten version where all math expressions are fully compatible for rendering in a notebook. Each step is laid out clearly with proper display-style math for effective visualization.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Graph Representation**\n",
    "\n",
    "A graph \\( G \\) is represented as:\n",
    "\n",
    "$$\n",
    "G = (V, E)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( V \\) is the set of nodes (vertices), \\( |V| = n \\),\n",
    "- \\( E \\subseteq V \\times V \\) is the set of edges.\n",
    "\n",
    "Node and edge features are represented as:\n",
    "- Node features: \\( \\mathbf{X} \\in \\mathbb{R}^{n \\times d_x} \\), where \\( d_x \\) is the dimension of each node feature.\n",
    "- Edge features: \\( \\mathbf{A} \\in \\mathbb{R}^{|E| \\times d_e} \\), where \\( d_e \\) is the dimension of each edge feature.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Bellman-Ford Updates**\n",
    "\n",
    "The Bellman-Ford algorithm iteratively updates the distance of each node from a source node \\( s \\). Its core equation is:\n",
    "\n",
    "$$\n",
    "d_u = \\min_{v \\in N(u)} \\big( d_v + w_{v \\to u} \\big)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( d_u \\): Distance to node \\( u \\),\n",
    "- \\( N(u) \\): Neighbors of \\( u \\),\n",
    "- \\( w_{v \\to u} \\): Weight of the edge between \\( v \\) and \\( u \\).\n",
    "\n",
    "In a GNN, this can be expressed using message-passing:\n",
    "\n",
    "1. **Message Aggregation**:\n",
    "   $$ \n",
    "   m_u^{(t)} = \\min_{v \\in N(u)} \\big( \\mathbf{h}_v^{(t-1)} + \\mathbf{w}_{v \\to u} \\big)\n",
    "   $$\n",
    "\n",
    "   Here:\n",
    "   - \\( \\mathbf{h}_v^{(t-1)} \\): Hidden state of node \\( v \\) from the previous layer (\\( t-1 \\)),\n",
    "   - \\( \\mathbf{w}_{v \\to u} \\): Edge weight or feature.\n",
    "\n",
    "2. **Node Update**:\n",
    "   $$ \n",
    "   \\mathbf{h}_u^{(t)} = \\sigma \\big( \\mathbf{W}_u \\cdot \\big[ \\mathbf{h}_u^{(t-1)} \\, \\Vert \\, m_u^{(t)} \\big] \\big)\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "   - \\( \\sigma \\): Non-linear activation (e.g., ReLU),\n",
    "   - \\( \\mathbf{W}_u \\): Learnable weight matrix,\n",
    "   - \\( \\Vert \\): Concatenation operator.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Encode-Process-Decode Framework**\n",
    "\n",
    "1. **Encoding**:\n",
    "   Raw node features \\( \\mathbf{x}_u \\) are mapped to a latent space:\n",
    "   $$\n",
    "   \\mathbf{h}_u^{(0)} = \\sigma \\big( \\mathbf{W}_{enc} \\cdot \\mathbf{x}_u \\big)\n",
    "   $$\n",
    "\n",
    "   Here, \\( \\mathbf{W}_{enc} \\) is the encoder's learnable weight matrix.\n",
    "\n",
    "2. **Processing**:\n",
    "   Perform \\( T \\) iterations of message-passing:\n",
    "   $$\n",
    "   \\mathbf{h}_u^{(t)} = \\sigma \\big( \\mathbf{W}_u \\cdot \\big[ \\mathbf{h}_u^{(t-1)} \\, \\Vert \\, m_u^{(t)} \\big] \\big)\n",
    "   $$\n",
    "\n",
    "3. **Decoding**:\n",
    "   After \\( T \\) iterations, the latent state \\( \\mathbf{h}_u^{(T)} \\) is decoded to produce the final prediction:\n",
    "   $$\n",
    "   \\hat{y}_u = \\sigma \\big( \\mathbf{W}_{dec} \\cdot \\mathbf{h}_u^{(T)} \\big)\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "   - \\( \\mathbf{W}_{dec} \\): Learnable weight matrix of the decoder,\n",
    "   - \\( \\hat{y}_u \\): Predicted value for node \\( u \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Reinforcement Learning (Actor-Critic)**\n",
    "\n",
    "#### **Policy Network (Actor):**\n",
    "The policy network outputs a probability distribution over actions \\( a \\):\n",
    "$$\n",
    "\\pi(a | \\mathbf{s}) = \\text{softmax} \\big( \\mathbf{W}_{actor} \\cdot \\mathbf{s} \\big)\n",
    "$$\n",
    "Where:\n",
    "- \\( \\mathbf{s} \\): Current state embedding,\n",
    "- \\( \\mathbf{W}_{actor} \\): Actor network's weight matrix.\n",
    "\n",
    "#### **Value Network (Critic):**\n",
    "The critic estimates the value of a state:\n",
    "$$\n",
    "V(\\mathbf{s}) = \\mathbf{W}_{critic} \\cdot \\mathbf{s}\n",
    "$$\n",
    "Where:\n",
    "- \\( \\mathbf{W}_{critic} \\): Critic network's weight matrix.\n",
    "\n",
    "#### **Loss Functions:**\n",
    "1. **Policy Loss**:\n",
    "   $$\n",
    "   \\mathcal{L}_{policy} = -\\sum_{t} \\log \\pi(a_t | \\mathbf{s}_t) \\cdot \\big( R_t - V(\\mathbf{s}_t) \\big)\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "   - \\( R_t \\): Discounted cumulative reward,\n",
    "   - \\( V(\\mathbf{s}_t) \\): Value estimate of the state.\n",
    "\n",
    "2. **Value Loss**:\n",
    "   $$\n",
    "   \\mathcal{L}_{value} = \\frac{1}{2} \\sum_{t} \\big( R_t - V(\\mathbf{s}_t) \\big)^2\n",
    "   $$\n",
    "\n",
    "3. **Total Loss**:\n",
    "   $$\n",
    "   \\mathcal{L} = \\mathcal{L}_{policy} + \\alpha \\cdot \\mathcal{L}_{value}\n",
    "   $$\n",
    "\n",
    "   Where \\( \\alpha \\) balances the two losses.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Supervised Learning**\n",
    "\n",
    "For supervised learning, predictions \\( \\hat{y}_u \\) are compared to ground-truth labels \\( y_u \\). The loss is binary cross-entropy:\n",
    "$$\n",
    "\\mathcal{L}_{BCE} = - \\frac{1}{N} \\sum_{i=1}^{N} \\big( y_i \\log \\hat{y}_i + (1 - y_i) \\log (1 - \\hat{y}_i) \\big)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Discounted Cumulative Returns**\n",
    "\n",
    "For reinforcement learning, discounted cumulative returns \\( R_t \\) are calculated as:\n",
    "$$\n",
    "R_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( r_{t+k} \\): Reward at time \\( t+k \\),\n",
    "- \\( \\gamma \\in [0, 1] \\): Discount factor for future rewards.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Flow**\n",
    "\n",
    "1. Input graph \\( G \\) includes:\n",
    "   - Node features \\( \\mathbf{X} \\),\n",
    "   - Edge features \\( \\mathbf{A} \\),\n",
    "   - Adjacency \\( E \\).\n",
    "2. Encode features:\n",
    "   $$\n",
    "   \\mathbf{h}_u^{(0)} = \\sigma(\\mathbf{W}_{enc} \\cdot \\mathbf{x}_u)\n",
    "   $$\n",
    "3. Propagate messages \\( T \\) times:\n",
    "   $$\n",
    "   m_u^{(t)} = \\min_{v \\in N(u)} \\big( \\mathbf{h}_v^{(t-1)} + \\mathbf{w}_{v \\to u} \\big)\n",
    "   $$\n",
    "   $$\n",
    "   \\mathbf{h}_u^{(t)} = \\sigma \\big( \\mathbf{W}_u \\cdot \\big[ \\mathbf{h}_u^{(t-1)} \\Vert m_u^{(t)} \\big] \\big)\n",
    "   $$\n",
    "4. Decode to predict:\n",
    "   $$\n",
    "   \\hat{y}_u = \\sigma(\\mathbf{W}_{dec} \\cdot \\mathbf{h}_u^{(T)})\n",
    "   $$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
